# Model Parameter Configuration

# Wide Parameters
# optimizer: one of {'`Adagrad`, `Adam`, `Ftrl`, `RMSProp`, `SGD`}
linear:
  linear_optimizer: 'Ftrl'
  wide_learning_rate: 0.1
  # regularization parameters, optional
  wide_l1: 0.5
  wide_l2: 1


# DNN Parameters
# connected_mode: one of {`simple`, `first_dense`, `last_dense`, `dense`, `resnet`}
#                 or arbitrary connections index tuples.
#   1. `simple`: normal dnn architecture.
#   2. `first_dense`: add addition connections from first input layer to all hidden layers.
#   3. `last_dense`: add addition connections from all previous layers to last layer.
#   4. `dense`: add addition connections between all layers, similar to DenseNet.
#   5. `resnet`: add addition connections between adjacent layers, similar to ResNet.
#   6. arbitrary connections list: add addition connections from layer_0 to layer_1 like 0-1.
#      eg: [0-1,0-3,1-2]  index start from zero(input_layer), max index is len(hidden_units), smaller index first.

# to use multi dnn model, set nested hidden_units, eg: [[1024,512,256], [512,256]]
# connected_mode can be set different for each dnn eg: ['simple', 'dense'] or use same mode if only set 'simple'
# only above 2 network architecture parameters can be different, other parameters are same for multi dnn model.
dnn:
  # network architecture
  hidden_units: [1024,512,256]
  connected_mode: 'simple'

  dnn_optimizer: 'Adagrad'
  deep_learning_rate: 0.1
  activation_function: 'tf.nn.relu'
  # regularization parameters, optional, set empty to be default None
  deep_l1: 0.01
  deep_l2: 0.01
  dropout:
  batch_normalization: 1 # bool


# CNN Parameters
cnn:
  # A flag to override the data format used in the model. channels_first
  # provides a performance boost on GPU but is not always compatible
  # with CPU. If left unspecified, the data format will be chosen
  # automatically based on whether TensorFlow was built for CPU or GPU.
  use_flag: 0
  data_format:
  height: 224
  width: 224
  num_channels: 3

  cnn_optimizer: 'Adagrad'
  weight_decay: 2e-4  # use 0.0002, performs better than 0.0001 that was originally suggested.
  momentum: 0.9
  num_iamges_train:
  num_iamges_test:
  use_distortion: 0

  # if use resnet
  resnet_size: 50 # choices: 18, 34, 50, 101, 152, 200


